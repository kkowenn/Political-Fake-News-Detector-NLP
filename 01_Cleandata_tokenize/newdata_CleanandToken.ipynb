{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "lFQkzE-jd-NZ",
        "outputId": "0409cfdf-24e2-461c-aecd-5a30a87dc315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "                                               title  \\\n",
            "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
            "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
            "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
            "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
            "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
            "\n",
            "                                                text subject  \\\n",
            "0  Donald Trump just couldn t wish all Americans ...    News   \n",
            "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
            "2  On Friday, it was revealed that former Milwauk...    News   \n",
            "3  On Christmas day, Donald Trump announced that ...    News   \n",
            "4  Pope Francis used his annual Christmas Day mes...    News   \n",
            "\n",
            "                date  label  \n",
            "0  December 31, 2017      1  \n",
            "1  December 31, 2017      1  \n",
            "2  December 30, 2017      1  \n",
            "3  December 29, 2017      1  \n",
            "4  December 25, 2017      1  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nbacktranslate = '/content/drive/MyDrive/deeplearningProgression/backtranslate.csv'\\nbacktranslate = pd.read_csv(backtranslate)\\n\\nhuggingFace_dataset = '/content/drive/MyDrive/deeplearningProgression/huggingface_dataset.csv'\\nhuggingFace_dataset = pd.read_csv(huggingFace_dataset)\\n\\n\\nprint(backtranslate.head())\\nprint(huggingFace_dataset.head())\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "newdata = '/content/drive/MyDrive/deeplearningProgression/Newdata.csv'\n",
        "newdata = pd.read_csv(newdata)\n",
        "\n",
        "print(newdata.head())\n",
        "\n",
        "'''\n",
        "backtranslate = '/content/drive/MyDrive/deeplearningProgression/backtranslate.csv'\n",
        "backtranslate = pd.read_csv(backtranslate)\n",
        "\n",
        "huggingFace_dataset = '/content/drive/MyDrive/deeplearningProgression/huggingface_dataset.csv'\n",
        "huggingFace_dataset = pd.read_csv(huggingFace_dataset)\n",
        "\n",
        "\n",
        "print(backtranslate.head())\n",
        "print(huggingFace_dataset.head())\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1jHW9GkFZijH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "\n",
        "# Download necessary NLTK models and corpora\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# 1. Remove any unnamed columns (if present)\n",
        "newdata = newdata.loc[:, ~newdata.columns.str.contains('^Unnamed')]\n",
        "\n",
        "# 2. Handle missing values by creating new columns and replacing missing values with \"missing\"\n",
        "newdata['news_missing'] = newdata['text'].isna() | newdata['text'].eq('')\n",
        "newdata['label_missing'] = newdata['label'].isna() | newdata['label'].eq('')\n",
        "\n",
        "newdata ['text'] = newdata['text'].replace('', 'missing')\n",
        "newdata ['text'] = newdata ['text'].fillna('missing')\n",
        "\n",
        "# 3. Convert all text to lowercase\n",
        "newdata ['text'] = newdata['text'].str.lower()\n",
        "\n",
        "# 4. Remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "newdata ['text'] = newdata ['text'].apply(remove_punctuation)\n",
        "\n",
        "# 5. Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    if isinstance(text, str):\n",
        "        word_tokens = word_tokenize(text)\n",
        "        filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "        return ' '.join(filtered_text)\n",
        "    return text\n",
        "\n",
        "newdata['text'] = newdata['text'].apply(remove_stopwords)\n",
        "\n",
        "# 6. Tokenization\n",
        "def tokenize_text(text):\n",
        "    return [word for word in word_tokenize(text) if word.isalnum()]\n",
        "\n",
        "newdata['text_tokens'] = newdata['text'].apply(tokenize_text)\n",
        "\n",
        "# 7. Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    if isinstance(text, str):\n",
        "        word_tokens = word_tokenize(text)\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(word.lower()) for word in word_tokens if word.lower() not in stop_words]\n",
        "        return ' '.join(lemmatized_tokens)\n",
        "    return text\n",
        "\n",
        "newdata['text'] = newdata['text'].apply(lemmatize_text)\n",
        "\n",
        "# Save the processed dataset to a new CSV file\n",
        "newdata.to_csv('new_dataset_processed.csv', index=False)\n",
        "\n",
        "# Check the quality of the cleaned dataset\n",
        "print(newdata.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76HEV9bBeV7F",
        "outputId": "e8c63d7b-2fe2-4f72-b2d3-610c1cefc17b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title  \\\n",
            "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
            "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
            "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
            "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
            "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
            "\n",
            "                                                text subject  \\\n",
            "0  donald trump wish american happy new year leav...    News   \n",
            "1  house intelligence committee chairman devin nu...    News   \n",
            "2  friday revealed former milwaukee sheriff david...    News   \n",
            "3  christmas day donald trump announced would bac...    News   \n",
            "4  pope francis used annual christmas day message...    News   \n",
            "\n",
            "                date  label  news_missing  label_missing  \\\n",
            "0  December 31, 2017      1         False          False   \n",
            "1  December 31, 2017      1         False          False   \n",
            "2  December 30, 2017      1         False          False   \n",
            "3  December 29, 2017      1         False          False   \n",
            "4  December 25, 2017      1         False          False   \n",
            "\n",
            "                                         text_tokens  \n",
            "0  [donald, trump, wish, americans, happy, new, y...  \n",
            "1  [house, intelligence, committee, chairman, dev...  \n",
            "2  [friday, revealed, former, milwaukee, sheriff,...  \n",
            "3  [christmas, day, donald, trump, announced, wou...  \n",
            "4  [pope, francis, used, annual, christmas, day, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each label\n",
        "label_counts = newdata['label'].value_counts()\n",
        "\n",
        "# Print the counts\n",
        "print(f\"Number of fake news (label=1): {label_counts[1]}\")\n",
        "print(f\"Number of true news (label=0): {label_counts[0]}\")\n"
      ],
      "metadata": {
        "id": "jSQMAyZ7jHN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee2da49a-eecc-4a1e-d9b7-0fffe9e7489f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of fake news (label=1): 23481\n",
            "Number of true news (label=0): 21417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fxDvDg1339qR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}