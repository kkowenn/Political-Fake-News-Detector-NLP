{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFQkzE-jd-NZ",
        "outputId": "ea598f38-2392-4087-a561-36d86a3091bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "                                               title  \\\n",
            "0  law enforcement high alert following threat co...   \n",
            "1                                            missing   \n",
            "2  unbelievable obamas attorney general say charl...   \n",
            "3  bobby jindal raised hindu us story christian c...   \n",
            "4  satan 2 russia unvelis image terrifying new su...   \n",
            "\n",
            "                                                text  label  title_missing  \\\n",
            "0  comment expected barack obama member fyf911 fu...      1          False   \n",
            "1                          post vote hillary already      1           True   \n",
            "2  demonstrator gathered last night exercising co...      1          False   \n",
            "3  dozen politically active pastor came private d...      0          False   \n",
            "4  rs28 sarmat missile dubbed satan 2 replace ss1...      1          False   \n",
            "\n",
            "   text_missing                                       title_tokens  \\\n",
            "0         False  ['law', 'enforcement', 'high', 'alert', 'follo...   \n",
            "1         False                                        ['missing']   \n",
            "2         False  ['unbelievable', 'obamas', 'attorney', 'genera...   \n",
            "3         False  ['bobby', 'jindal', 'raised', 'hindu', 'uses',...   \n",
            "4         False  ['satan', '2', 'russia', 'unvelis', 'image', '...   \n",
            "\n",
            "                                         text_tokens  \n",
            "0  ['comment', 'expected', 'barack', 'obama', 'me...  \n",
            "1            ['post', 'votes', 'hillary', 'already']  \n",
            "2  ['demonstrators', 'gathered', 'last', 'night',...  \n",
            "3  ['dozen', 'politically', 'active', 'pastors', ...  \n",
            "4  ['rs28', 'sarmat', 'missile', 'dubbed', 'satan...  \n",
            "   Unnamed: 0                                               news  label\n",
            "0           0  Pennsylvania is under a court order to count t...      0\n",
            "1           1  Biden and Democrats have dismantled border sec...      0\n",
            "2           2  Katie Hobbs â€œhas voted to double our gas tax. ...      1\n",
            "3           3  Reuters reported that Nancy Pelosi bought 10 m...      0\n",
            "4           4  It's not true that the United States was built...      0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "kaggle_dataset = '/content/drive/MyDrive/deeplearningProgression/kaggle_dataset.csv'\n",
        "kaggle_dataset = pd.read_csv(kaggle_dataset)\n",
        "\n",
        "huggingFace_dataset = '/content/drive/MyDrive/deeplearningProgression/huggingface_dataset_fixed.csv'\n",
        "huggingFace_dataset = pd.read_csv(huggingFace_dataset)\n",
        "\n",
        "\n",
        "print(kaggle_dataset.head())\n",
        "print(huggingFace_dataset.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "\n",
        "# Download necessary NLTK models and corpora\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# 1. Remove any unnamed columns (if present)\n",
        "huggingFace_dataset = huggingFace_dataset.loc[:, ~huggingFace_dataset.columns.str.contains('^Unnamed')]\n",
        "\n",
        "# 2. Handle missing values by creating new columns and replacing missing values with \"missing\"\n",
        "huggingFace_dataset['news_missing'] = huggingFace_dataset['news'].isna() | huggingFace_dataset['news'].eq('')\n",
        "huggingFace_dataset['label_missing'] = huggingFace_dataset['label'].isna() | huggingFace_dataset['label'].eq('')\n",
        "\n",
        "huggingFace_dataset['news'] = huggingFace_dataset['news'].replace('', 'missing')\n",
        "huggingFace_dataset['news'] = huggingFace_dataset['news'].fillna('missing')\n",
        "\n",
        "# 3. Convert all text to lowercase\n",
        "huggingFace_dataset['news'] = huggingFace_dataset['news'].str.lower()\n",
        "\n",
        "# 4. Remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "huggingFace_dataset['news'] = huggingFace_dataset['news'].apply(remove_punctuation)\n",
        "\n",
        "# 5. Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    if isinstance(text, str):\n",
        "        word_tokens = word_tokenize(text)\n",
        "        filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "        return ' '.join(filtered_text)\n",
        "    return text\n",
        "\n",
        "huggingFace_dataset['news'] = huggingFace_dataset['news'].apply(remove_stopwords)\n",
        "\n",
        "# 6. Tokenization\n",
        "def tokenize_text(text):\n",
        "    return [word for word in word_tokenize(text) if word.isalnum()]\n",
        "\n",
        "huggingFace_dataset['news_tokens'] = huggingFace_dataset['news'].apply(tokenize_text)\n",
        "\n",
        "# 7. Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    if isinstance(text, str):\n",
        "        word_tokens = word_tokenize(text)\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(word.lower()) for word in word_tokens if word.lower() not in stop_words]\n",
        "        return ' '.join(lemmatized_tokens)\n",
        "    return text\n",
        "\n",
        "huggingFace_dataset['news'] = huggingFace_dataset['news'].apply(lemmatize_text)\n",
        "\n",
        "# Save the processed dataset to a new CSV file\n",
        "huggingFace_dataset.to_csv('huggingface_dataset_processed.csv', index=False)\n",
        "\n",
        "# Check the quality of the cleaned dataset\n",
        "print(huggingFace_dataset.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76HEV9bBeV7F",
        "outputId": "c185743a-da8d-4686-dcd9-71b92c8cc2fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                news  label  news_missing  \\\n",
            "0  pennsylvania court order count ballot election...      0         False   \n",
            "1  biden democrat dismantled border security neva...      0         False   \n",
            "2  katie hobbs voted double gas tax runup midterm...      1         False   \n",
            "3  reuters reported nancy pelosi bought 10 millio...      0         False   \n",
            "4  true united state built stolen land florida go...      0         False   \n",
            "\n",
            "   label_missing                                        news_tokens  \n",
            "0          False  [pennsylvania, court, order, count, ballots, e...  \n",
            "1          False  [biden, democrats, dismantled, border, securit...  \n",
            "2          False  [katie, hobbs, voted, double, gas, tax, runup,...  \n",
            "3          False  [reuters, reported, nancy, pelosi, bought, 10,...  \n",
            "4          False  [true, united, states, built, stolen, land, fl...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "huggingFace_dataset.to_csv('huggingface_dataset.csv', index=False)"
      ],
      "metadata": {
        "id": "0J3FPo5cgQRB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSQMAyZ7jHN5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}