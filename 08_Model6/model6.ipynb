{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "US6IvBjreyFv",
        "outputId": "0234bfd2-41cd-4b20-fb8d-daa15bad40bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import library"
      ],
      "metadata": {
        "id": "gqtL9w6qesDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, Input\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import pickle"
      ],
      "metadata": {
        "id": "6zpcR1mQexBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNe4uqBve1Jj",
        "outputId": "bd83f778-b555-4f6f-c166-d0dd9eff6365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKTV-cRejZv-",
        "outputId": "3775b84a-4a7a-4a05-9011-e0d8d0330c1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "set1\n",
            "                                               title  \\\n",
            "0  law enforcement high alert following threat co...   \n",
            "1                                            missing   \n",
            "2  unbelievable obamas attorney general say charl...   \n",
            "3  bobby jindal raised hindu us story christian c...   \n",
            "4  satan 2 russia unvelis image terrifying new su...   \n",
            "\n",
            "                                                text  label  title_missing  \\\n",
            "0  comment expected barack obama member fyf911 fu...      1          False   \n",
            "1                          post vote hillary already      1           True   \n",
            "2  demonstrator gathered last night exercising co...      1          False   \n",
            "3  dozen politically active pastor came private d...      0          False   \n",
            "4  rs28 sarmat missile dubbed satan 2 replace ss1...      1          False   \n",
            "\n",
            "   text_missing                                       title_tokens  \\\n",
            "0         False  ['law', 'enforcement', 'high', 'alert', 'follo...   \n",
            "1         False                                        ['missing']   \n",
            "2         False  ['unbelievable', 'obamas', 'attorney', 'genera...   \n",
            "3         False  ['bobby', 'jindal', 'raised', 'hindu', 'uses',...   \n",
            "4         False  ['satan', '2', 'russia', 'unvelis', 'image', '...   \n",
            "\n",
            "                                         text_tokens  \n",
            "0  ['comment', 'expected', 'barack', 'obama', 'me...  \n",
            "1            ['post', 'votes', 'hillary', 'already']  \n",
            "2  ['demonstrators', 'gathered', 'last', 'night',...  \n",
            "3  ['dozen', 'politically', 'active', 'pastors', ...  \n",
            "4  ['rs28', 'sarmat', 'missile', 'dubbed', 'satan...  \n",
            "set2\n",
            "                                               title  \\\n",
            "0  law enforcement high alert following threat co...   \n",
            "1                                            missing   \n",
            "2  unbelievable obamas attorney general say charl...   \n",
            "3  bobby jindal raised hindu us story christian c...   \n",
            "4  satan 2 russia unvelis image terrifying new su...   \n",
            "\n",
            "                                                text  label  title_missing  \\\n",
            "0  comment expected barack obama member fyf911 fu...      1          False   \n",
            "1                          post vote hillary already      1           True   \n",
            "2  demonstrator gathered last night exercising co...      1          False   \n",
            "3  dozen politically active pastor came private d...      0          False   \n",
            "4  rs28 sarmat missile dubbed satan 2 replace ss1...      1          False   \n",
            "\n",
            "   text_missing                                       title_tokens  \\\n",
            "0         False  ['law', 'enforcement', 'high', 'alert', 'follo...   \n",
            "1         False                                        ['missing']   \n",
            "2         False  ['unbelievable', 'obamas', 'attorney', 'genera...   \n",
            "3         False  ['bobby', 'jindal', 'raised', 'hindu', 'uses',...   \n",
            "4         False  ['satan', '2', 'russia', 'unvelis', 'image', '...   \n",
            "\n",
            "                                         text_tokens  \\\n",
            "0  ['comment', 'expected', 'barack', 'obama', 'me...   \n",
            "1            ['post', 'votes', 'hillary', 'already']   \n",
            "2  ['demonstrators', 'gathered', 'last', 'night',...   \n",
            "3  ['dozen', 'politically', 'active', 'pastors', ...   \n",
            "4  ['rs28', 'sarmat', 'missile', 'dubbed', 'satan...   \n",
            "\n",
            "                                back_translated_text  \\\n",
            "0  wait comment barack obama member fyf911 fukyof...   \n",
            "1                                      voted hillary   \n",
            "2  protester gathered last night exercise constit...   \n",
            "3  dozen politically active pastor came private d...   \n",
            "4  rs28 sarmat missile called satan 2 replaces ss...   \n",
            "\n",
            "   back_translated_text_missing  \\\n",
            "0                         False   \n",
            "1                         False   \n",
            "2                         False   \n",
            "3                         False   \n",
            "4                         False   \n",
            "\n",
            "                         back_translated_text_tokens  \n",
            "0  ['wait', 'comment', 'barack', 'obama', 'member...  \n",
            "1                               ['voted', 'hillary']  \n",
            "2  ['protester', 'gathered', 'last', 'night', 'ex...  \n",
            "3  ['dozen', 'politically', 'active', 'pastor', '...  \n",
            "4  ['rs28', 'sarmat', 'missile', 'called', 'satan...  \n",
            "set3\n",
            "                                                news  label  news_missing  \\\n",
            "0  pennsylvania court order count ballot election...      0         False   \n",
            "1  biden democrat dismantled border security neva...      0         False   \n",
            "2  katie hobbs voted double gas tax runup midterm...      1         False   \n",
            "3  reuters reported nancy pelosi bought 10 millio...      0         False   \n",
            "4  true united state built stolen land florida go...      0         False   \n",
            "\n",
            "   label_missing                                        news_tokens  \n",
            "0          False  ['pennsylvania', 'court', 'order', 'count', 'b...  \n",
            "1          False  ['biden', 'democrats', 'dismantled', 'border',...  \n",
            "2          False  ['katie', 'hobbs', 'voted', 'double', 'gas', '...  \n",
            "3          False  ['reuters', 'reported', 'nancy', 'pelosi', 'bo...  \n",
            "4          False  ['true', 'united', 'states', 'built', 'stolen'...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "kaggle_dataset = '/content/drive/MyDrive/deeplearningProgression/kaggle_dataset.csv'\n",
        "kaggle_dataset = pd.read_csv(kaggle_dataset)\n",
        "\n",
        "backtranslate = '/content/drive/MyDrive/deeplearningProgression/backtranslate_processed.csv'\n",
        "backtranslate = pd.read_csv(backtranslate)\n",
        "\n",
        "huggingFace_dataset = '/content/drive/MyDrive/deeplearningProgression/huggingface_dataset.csv'\n",
        "huggingFace_dataset = pd.read_csv(huggingFace_dataset)\n",
        "\n",
        "print(\"set1\")\n",
        "print(kaggle_dataset.head())\n",
        "print(\"set2\")\n",
        "print(backtranslate.head())\n",
        "print(\"set3\")\n",
        "print(huggingFace_dataset.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all entries in 'back_translated_text' are strings\n",
        "backtranslate['back_translated_text_tokens'] = backtranslate['back_translated_text_tokens'].astype(str).fillna('')"
      ],
      "metadata": {
        "id": "3zXKvaq5tCLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to augment with synonym replacement\n",
        "def synonym_replacement(text, n=1):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))\n",
        "    random.shuffle(random_word_list)\n",
        "    num_replaced = 0\n",
        "    for random_word in random_word_list:\n",
        "        synonyms = wordnet.synsets(random_word)\n",
        "        if len(synonyms) >= 1:\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            new_words = [synonym if word == random_word else word for word in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "# Function to augment with random insertion\n",
        "def random_insertion(words, n):\n",
        "    words = words.split()\n",
        "    new_words = words.copy()\n",
        "    for _ in range(n):\n",
        "        add_word(new_words)\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def add_word(new_words):\n",
        "    synonyms = []\n",
        "    counter = 0\n",
        "    while len(synonyms) < 1:\n",
        "        random_word = new_words[random.randint(0, len(new_words)-1)]\n",
        "        synonyms = get_synonyms(random_word)\n",
        "        counter += 1\n",
        "        if counter >= 10:\n",
        "            return\n",
        "    random_synonym = synonyms[0]\n",
        "    random_idx = random.randint(0, len(new_words)-1)\n",
        "    new_words.insert(random_idx, random_synonym)\n",
        "\n",
        "# Random Deletion augmentation function\n",
        "def random_deletion(words, p):\n",
        "    words = words.split()\n",
        "    if len(words) == 1:\n",
        "        return words\n",
        "    new_words = [word for word in words if random.uniform(0, 1) > p]\n",
        "    if len(new_words) == 0:\n",
        "        return [random.choice(words)]\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "# Function to get synonyms for random insertion\n",
        "def get_synonyms(word):\n",
        "    synonyms = wordnet.synsets(word)\n",
        "    return [syn.lemmas()[0].name() for syn in synonyms] if synonyms else []\n",
        "\n",
        "# Apply augmentation to Kaggle dataset\n",
        "kaggle_dataset['text_tokens_synonym'] = kaggle_dataset['text_tokens'].apply(lambda x: synonym_replacement(x))\n",
        "kaggle_dataset['text_tokens_insertion'] = kaggle_dataset['text_tokens'].apply(lambda x: random_insertion(x, 2))\n",
        "kaggle_dataset['text_tokens_deletion'] = kaggle_dataset['text_tokens'].apply(lambda x: random_deletion(x, 0.2))\n",
        "\n",
        "# Apply augmentation to back-translated dataset (handling missing or non-string values)\n",
        "backtranslate['back_translated_text_synonym'] = backtranslate['back_translated_text_tokens'].apply(lambda x: synonym_replacement(x))\n",
        "backtranslate['back_translated_text_insertion'] = backtranslate['back_translated_text_tokens'].apply(lambda x: random_insertion(x, 2))\n",
        "backtranslate['back_translated_text_deletion'] = backtranslate['back_translated_text_tokens'].apply(lambda x: random_deletion(x, 0.2))\n",
        "\n",
        "# Combine original and augmented datasets\n",
        "x_train_original = kaggle_dataset['text_tokens'].values.tolist() + backtranslate['back_translated_text_tokens'].values.tolist()\n",
        "x_train_synonym = kaggle_dataset['text_tokens_synonym'].values.tolist() + backtranslate['back_translated_text_synonym'].values.tolist()\n",
        "x_train_insertion = kaggle_dataset['text_tokens_insertion'].values.tolist() + backtranslate['back_translated_text_insertion'].values.tolist()\n",
        "x_train_deletion = kaggle_dataset['text_tokens_deletion'].values.tolist() + backtranslate['back_translated_text_deletion'].values.tolist()\n",
        "y_train = np.concatenate([kaggle_dataset['label'].values, backtranslate['label'].values])\n",
        "\n",
        "# Concatenate original and augmented data\n",
        "x_train_combined = x_train_original + x_train_synonym + x_train_insertion + x_train_deletion\n",
        "y_train_combined = np.concatenate([y_train, y_train, y_train, y_train])\n",
        "\n",
        "# Display the length of the combined dataset\n",
        "print(f\"Length of combined dataset (original + augmented): {len(x_train_combined)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzfclVxGrS3p",
        "outputId": "c500c10b-0175-4ac9-ed53-6ded3b9ca890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of combined dataset (original + augmented): 408516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train_combined\n",
        "y_train = y_train_combined\n",
        "\n",
        "# Preprocessing HuggingFace dataset for testing\n",
        "x_test = huggingFace_dataset['news_tokens'].values.tolist()\n",
        "y_test = huggingFace_dataset['label'].values\n",
        "\n",
        "# Display the length of the combined dataset\n",
        "print(f\"Length of combined dataset (back translate): {len(x_train)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZboeN-okwGA",
        "outputId": "6f2ed9fa-91ac-4f82-e2dc-95a08e377d96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of combined dataset (back translate): 408516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization and padding"
      ],
      "metadata": {
        "id": "7RXc0S1Uf4yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Ensure all values in x_train are strings and handle NaN values\n",
        "x_train = [str(text) if not pd.isnull(text) else \"\" for text in x_train]\n",
        "x_test = [str(text) if not pd.isnull(text) else \"\" for text in x_test]\n",
        "\n",
        "# Tokenization and padding\n",
        "total_words = 10000  # Limit to top 10,000 words\n",
        "tokenizer = Tokenizer(num_words=total_words)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "# Save the tokenizer for later use in the Streamlit app\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Convert texts to sequences of integers\n",
        "x_train_sequences = tokenizer.texts_to_sequences(x_train)\n",
        "x_test_sequences = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# Padding sequences to ensure uniform length\n",
        "max_len_of_news = 200  # Set a fixed length for padding\n",
        "x_train_padded = pad_sequences(x_train_sequences, maxlen=max_len_of_news, padding='post', truncating='post')\n",
        "x_test_padded = pad_sequences(x_test_sequences, maxlen=max_len_of_news, padding='post', truncating='post')\n",
        "\n",
        "# Confirm that the data is padded and tokenized correctly\n",
        "print(f\"x_train_padded shape: {x_train_padded.shape}\")\n",
        "print(f\"x_test_padded shape: {x_test_padded.shape}\")\n"
      ],
      "metadata": {
        "id": "d1FnbZS6kics",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c006e2-6b65-4ba4-8483-bce21a96bfc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train_padded shape: (408516, 200)\n",
            "x_test_padded shape: (21318, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model architecture"
      ],
      "metadata": {
        "id": "c9IPRdcagJJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Define the model with reduced complexity\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding layer\n",
        "model.add(Embedding(input_dim=total_words, output_dim=128))\n",
        "\n",
        "# Reduced LSTM units to 8 and dropped one LSTM layer\n",
        "model.add(Bidirectional(LSTM(8, return_sequences=True)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))  # Reduced dropout to 0.5\n",
        "\n",
        "# Second LSTM layer (reduced units)\n",
        "model.add(Bidirectional(LSTM(8)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Dense layer with reduced units (from 64 to 32)\n",
        "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))  # Keep dropout at 0.5\n",
        "\n",
        "# Final dense layer with sigmoid activation\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "p_wX-bx4gLVZ",
        "outputId": "76cb4e33-3e1f-419e-c59e-beb4c8e6a07c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "trainning"
      ],
      "metadata": {
        "id": "7NqoZPGLgSTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Learning rate scheduler and early stopping\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.00001, verbose=1)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint(filepath='model_weights_best.keras', save_best_only=True)\n",
        "\n",
        "# Model summary to confirm changes\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train_padded, y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=16,  # Smaller batch size\n",
        "                    validation_split=0.1,\n",
        "                    callbacks=[checkpoint, early_stop, reduce_lr])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "id": "Uad1lN1bgRMB",
        "outputId": "b6e5e016-9760-4b7b-fc61-35c9e56d6e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m22979/22979\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m708s\u001b[0m 30ms/step - accuracy: 0.8926 - loss: 0.3174 - val_accuracy: 0.9626 - val_loss: 0.1046 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m22979/22979\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m696s\u001b[0m 30ms/step - accuracy: 0.9738 - loss: 0.0898 - val_accuracy: 0.9704 - val_loss: 0.0996 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m22979/22979\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m738s\u001b[0m 30ms/step - accuracy: 0.9847 - loss: 0.0580 - val_accuracy: 0.9776 - val_loss: 0.0781 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m22979/22979\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m745s\u001b[0m 30ms/step - accuracy: 0.9908 - loss: 0.0375 - val_accuracy: 0.9807 - val_loss: 0.0689 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m22979/22979\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 30ms/step - accuracy: 0.9935 - loss: 0.0280 - val_accuracy: 0.9830 - val_loss: 0.0665 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m 5533/22979\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:16\u001b[0m 28ms/step - accuracy: 0.9957 - loss: 0.0185"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "save the model & evalution"
      ],
      "metadata": {
        "id": "bU5oXV14hL0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Save the entire model after training is complete for use in the Streamlit app\n",
        "model.save('model5.h5')\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, accuracy = model.evaluate(x_test_padded, y_test)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "# Predict and print classification report\n",
        "y_pred = model.predict(x_test_padded)\n",
        "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
        "print(classification_report(y_test, y_pred_classes, target_names=['Fake', 'Real']))"
      ],
      "metadata": {
        "id": "NCOB0YEIgXDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Compute and plot the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wFuOFLaxgZSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation accuracy and loss\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')"
      ],
      "metadata": {
        "id": "TzyyXbL8gazu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6P4i8426nkiE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}